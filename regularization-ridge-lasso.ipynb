{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Introduction to Regularization\n",
    "\n",
    "_Authors: Kiefer Katovich (SF)_\n",
    "\n",
    "_Modifications for DSI-East by Justin Pounders_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Overview of regularization](#intro)\n",
    "- [What is the effect of regularization](#effect)\n",
    "- [Standardizing predictors is required](#standardization)\n",
    "- [Visualizing the Ridge](#viz-ridge)\n",
    "- [Visualizing the Lasso](#viz-lasso)\n",
    "- [Visualizing the Elastic-Net](#viz-elastic-net)\n",
    "- [Model performance with complex predictor matrix using regularization](#model-performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Overview of regularization\n",
    "\n",
    "---\n",
    "\n",
    "The goal of \"regularizing\" regression models is to structurally prevent overfitting by imposing a penalty on the coefficients of the model.\n",
    "\n",
    "Regularization methods like the Ridge and Lasso add this additional \"penalty\" on the size of coefficients to the loss function. When the loss function is minimized, this additional component is added to the residual sum of squares.\n",
    "\n",
    "In other words, the minimization becomes a balance between the error between predictions and true values and the size of the coefficients. \n",
    "\n",
    "The two most common types of regularization are the **Lasso**, **Ridge**. There is a mixture of them called the **Elastic Net**. We will take a look at the mathematics of regularization and the effect these penalties have on model fits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='effect'></a>\n",
    "\n",
    "## What is the effect of regularization?\n",
    "\n",
    "---\n",
    "\n",
    "**To demonstrate the effects we will be using a dataset on wine quality.**\n",
    "\n",
    "An important aspect of this data, which is a reason why we might choose to use regularization, is that there is **multicollinearity** in the data. The term multicollinearity means that there are high correlations between predictor variables in your model. \n",
    "\n",
    "**This can lead to a variety of problems including:**\n",
    "1. The effect of predictor variables estimated by your regression will depend on what other variabes are included in your model.\n",
    "2. Predictors can have wildly different effects depending on the observations in your sample, and small changes in samples can result in very different estimated effects.\n",
    "3. With very high multicollinearity, the inverse matrix the computer calculates may not be accurate.\n",
    "4. We can no longer interpret a coefficient on a variable as the effect on the target of a one unit increase in that variable holding the other variables constant. This is because when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n",
    "\n",
    "The Ridge is best suited to deal with multicollinearity. Lasso also deals with multicollinearity between variables, but in a more brutal way (it \"zeroes out\" the less effective variable).\n",
    "\n",
    "The Lasso is particularly useful when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you by forcing coefficients to be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the wine csv\n",
    "\n",
    "This version has red and white wines concatenated together and tagged with a binary 1,0 indicator (1 is red wine). There are many other variables purportedly related to the rated quality of the wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv('./datasets/winequality_merged.csv')\n",
    "\n",
    "# replace spaces in column names and convert all columns to lowercase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at the correlation between variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix can be hard to take in if you have a lot of features.  Let's visualize it.\n",
    "\n",
    "**Note**: this is a common visualization in EDA building up to modeling.  You may want to stash this code cell to a \"cheat sheet\"/recipe file for future use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the default matplotlib figure size to 7x7:\n",
    "fix, ax = plt.subplots(figsize=(9,7))\n",
    "\n",
    "# Generate a mask for the upper triangle (taken from seaborn example gallery)\n",
    "mask = np.zeros_like(wine_corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Plot the heatmap with seaborn.\n",
    "# Assign the matplotlib axis the function returns. This will let us resize the labels.\n",
    "ax = sns.heatmap(wine_corr, mask=mask, ax=ax)\n",
    "\n",
    "# Resize the labels.\n",
    "ax.set_xticklabels(ax.xaxis.get_ticklabels(), fontsize=14)\n",
    "ax.set_yticklabels(ax.yaxis.get_ticklabels(), fontsize=14)\n",
    "\n",
    "# If you put plt.show() at the bottom, it prevents those useless printouts from matplotlib.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='standardization'></a>\n",
    "\n",
    "## Standardizing predictors is required\n",
    "\n",
    "---\n",
    "\n",
    "With the Lasso and Ridge it is neccessary to standardize the predictor columns before constructing the models, even the dummy coded categorical variables. \n",
    "\n",
    "Below we define our target variable and then normalize the columns that are not the target.\n",
    "\n",
    "Recall the equations for the Ridge and Lasso penalties:\n",
    "\n",
    "### $$ \\text{Ridge penalty}\\; = \\alpha \\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "### $$ \\text{Lasso penalty}\\; = \\alpha \\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**Check: How are the $\\beta$ coefficients affected by the mean and variance of your variables?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Choose a target/dependent variable that we will predict\n",
    "target = 'quality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select all the columns that are not the target\n",
    "\n",
    "\n",
    "# By subtracting the mean and dividing by the standard devation, the normalization procedure is putting \n",
    "# all of the predictor variables on the same scale (distributions with mean == 0 and standard deviation == 1).\n",
    "# Do this in Pandas.\n",
    "# (Yes, we could use sklearn's StandardScaler as well.  Try it and see if you get the same answer!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the target and predictor matrix \n",
    "\n",
    "We will be making two X, Y datasets. One of them will be dramatically more complex (and thus overfit).\n",
    "\n",
    "1. The first is going to just be all the variables added together.\n",
    "2. The second will have all interactions between the variables included. This should overfit the target quite a bit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With patsy (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import patsy\n",
    "\n",
    "# Get the non-target cols with a simple list comprehension\n",
    "non_target_cols = [c for c in wine.columns if c != target]\n",
    "print('Non-target columns are...')\n",
    "print(non_target_cols)\n",
    "print()\n",
    "\n",
    "# Use some string adding and joining to make the simple model formula:\n",
    "formula_simple = target + ' ~ ' + ' + '.join(non_target_cols) + ' -1'\n",
    "print('The \"simple\" patsy formula is...')\n",
    "print(formula_simple)\n",
    "print()\n",
    "\n",
    "# Make the complex formula:\n",
    "formula_complex = target + ' ~ (' + ' + '.join(non_target_cols) + ')**2 -1'\n",
    "print('The \"complex\" patsy formula is...')\n",
    "print(formula_complex)\n",
    "print()\n",
    "\n",
    "# Create the X and Y pairs for both!\n",
    "Y, X = patsy.dmatrices(formula_simple, data=wine, return_type='dataframe')\n",
    "Yoverfit, Xoverfit = patsy.dmatrices(formula_complex, data=wine, return_type='dataframe')\n",
    "\n",
    "Y = Y.values.ravel()\n",
    "Yoverfit = Yoverfit.values.ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Look at how the shapes of the X matrices differ. You can see that the Xoverfit has considerably more columns due to the creation of all possible two-way interactions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> **Note**: From here on there is a lot of code that is \"nice-to-know\" if you want to revisit later.  It is useful and will also help you brush up on Python.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-ridge'></a>\n",
    "\n",
    "## Visualizing the Ridge\n",
    "\n",
    "---\n",
    "\n",
    "Import the `Ridge` model class from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function iterates over a series of different alpha regularization parameters. \n",
    "\n",
    "The function stores the results of the model so that we can plot them interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_coefs(X, Y, alphas):\n",
    "    \n",
    "    # set up the list to hold the different sets of coefficients:\n",
    "    coefs = []\n",
    "    \n",
    "    # Set up a ridge regression object\n",
    "    ridge_reg = Ridge()\n",
    "    \n",
    "    # Iterate through the alphas fed into the function:\n",
    "    for a in alphas:\n",
    "        \n",
    "        # On each alpha reset the ridge model's alpha to the current one:\n",
    "        ridge_reg.set_params(alpha=a)\n",
    "        \n",
    "        # fit or refit the model on the provided X, Y\n",
    "        ridge_reg.fit(X, Y)\n",
    "        \n",
    "        # Get out the coefficient list\n",
    "        coefs.append(ridge_reg.coef_)\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha values for the ridge are best visualized on a logarithmic \"magnitude\" scale. Essentially, the effect of alpha on the coefficients does not increase linearly but by orders of magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\n",
    "r_alphas = np.logspace(0, 5, 200)\n",
    "\n",
    "# Get the coefficients for each alpha for the Ridge, using the function above\n",
    "r_coefs = ridge_coefs(X, Y, r_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plotting function below will:\n",
    "\n",
    " - Plot the effect of changing alpha on the coefficient size on a **path** graph\n",
    " - Plot the effect of changing alpha on the coefficient size on a **bar** graph\n",
    " \n",
    "Each one gives informative information. It's just two different ways of visualizing the same thing. The chart is interactive so you can play around with the values of alpha across the specified range above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The cycler package lets us \"cycle\" throug colors.\n",
    "# Just another thing i had to look up on stackoverflow. That's my life.\n",
    "from cycler import cycler\n",
    "\n",
    "def coef_plotter(alphas, coefs, feature_names, to_alpha, regtype='ridge'):\n",
    "    \n",
    "    # Get the full range of alphas before subsetting to keep the plots from \n",
    "    # resetting axes each time. (We use these values to set static axes later).\n",
    "    amin = np.min(alphas)\n",
    "    amax = np.max(alphas)\n",
    "    \n",
    "    # Subset the alphas and coefficients to just the ones below the set limit\n",
    "    # from the interactive widget:\n",
    "    alphas = [a for a in alphas if a <= to_alpha]\n",
    "    coefs = coefs[0:len(alphas)]\n",
    "    \n",
    "    # Get some colors from seaborn:\n",
    "    colors = sns.color_palette(\"husl\", len(coefs[0]))\n",
    "    \n",
    "    # Get the figure and reset the size to be wider:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,5)\n",
    "\n",
    "    # We have two axes this time on our figure. \n",
    "    # The fig.add_subplot adds axes to our figure. The number inside stands for:\n",
    "    #[figure_rows|figure_cols|position_of_current_axes]\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    \n",
    "    # Give it the color cycler:\n",
    "    ax1.set_prop_cycle(cycler('color', colors))\n",
    "    \n",
    "    # Print a vertical line showing our current alpha threshold:\n",
    "    ax1.axvline(to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot the lines of the alphas on x-axis and coefficients on y-axis\n",
    "    ax1.plot(alphas, coefs, lw=2)\n",
    "    \n",
    "    # set labels for axes:\n",
    "    ax1.set_xlabel('alpha', fontsize=20)\n",
    "    ax1.set_ylabel('coefficients', fontsize=20)\n",
    "    \n",
    "    # If this is for the ridge, set this to a log scale on the x-axis:\n",
    "    if regtype == 'ridge':\n",
    "        ax1.set_xscale('log')\n",
    "    \n",
    "    # Enforce the axis limits:\n",
    "    ax1.set_xlim([amin, amax])\n",
    "    \n",
    "    # Put a title on the axis\n",
    "    ax1.set_title(regtype+' coef paths\\n', fontsize=20)\n",
    "    \n",
    "    # Get the ymin and ymax for this axis to enforce it to be the same on the \n",
    "    # second chart:\n",
    "    ymin, ymax = ax1.get_ylim()\n",
    "\n",
    "    # Add our second axes for the barplot in position 2:\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    \n",
    "    # Position the bars according to their index from the feature names variable:\n",
    "    ax2.bar(list(range(1, len(feature_names)+1)), coefs[-1], align='center', color=colors)\n",
    "    ax2.set_xticks(list(range(1, len(feature_names)+1)))\n",
    "    \n",
    "    # Reset the ticks from numbers to acutally be the names:\n",
    "    ax2.set_xticklabels(feature_names, rotation=65, fontsize=12)\n",
    "    \n",
    "    # enforce limits and add titles, labels\n",
    "    ax2.set_ylim([ymin, ymax])\n",
    "    ax2.set_title(regtype+' predictor coefs\\n', fontsize=20)\n",
    "    ax2.set_xlabel('coefficients', fontsize=20)\n",
    "    ax2.set_ylabel('alpha', fontsize=20)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the ipython widgets so we can make this plotting function interactive!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ipywidgets import *\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function and `interact` from ipywidgets lets me take some specified alphas that we have already calculated the coefficients for and plot them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_plot_runner(log_of_alpha=0):\n",
    "    coef_plotter(r_alphas, r_coefs, X.columns, 10**log_of_alpha, regtype='ridge')\n",
    "\n",
    "interact(ridge_plot_runner, log_of_alpha=(0.0,5.0,0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-lasso'></a>\n",
    "\n",
    "## Visualizing the Lasso\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def lasso_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    lasso_reg = Lasso()\n",
    "    for a in alphas:\n",
    "        lasso_reg.set_params(alpha=a)\n",
    "        lasso_reg.fit(X, Y)\n",
    "        coefs.append(lasso_reg.coef_)\n",
    "        \n",
    "    return coefs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphas for the Lasso tend to effect regularization linearly rather than by orders of magnitude like in the ridge. \n",
    "\n",
    "A linear series of alphas is sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_alphas = np.arange(0.001, 0.15, 0.0025)\n",
    "l_coefs = lasso_coefs(X, Y, l_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the same plotting function above, but now with the calculated coefficients of alpha for the Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lasso_plot_runner(alpha=0):\n",
    "    coef_plotter(l_alphas, l_coefs, X.columns, alpha, regtype='lasso')\n",
    "\n",
    "interact(lasso_plot_runner, alpha=(0.001,0.2,0.0025))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='viz-elastic-net'></a>\n",
    "\n",
    "## Visualizing the Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "Recall that the Elastic Net combines the Ridge and Lasso penalties.\n",
    "\n",
    "`ElasticNet` in sklearn has two parameters:\n",
    "- `alpha`: the regularization strength.\n",
    "- `l1_ratio`: the amount of L1 vs L2 penalty. An l1_ratio of 0 is equivalent to the Ridge, whereas an l1_ratio of 1 is equivalent to the Lasso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the coefficients with both alpha values and an l1_ratio of 0.05. Lasso can \"overpower\" the Ridge penalty in some datasets, and so rather than an equal balance I'm just adding a little bit of Lasso in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the same as the ridge coefficient by alpha calculator\n",
    "def elasticnet_coefs(X, Y, alphas):\n",
    "    coefs = []\n",
    "    enet_reg = ElasticNet()\n",
    "    for a in alphas:\n",
    "        enet_reg.set_params(alpha=a, l1_ratio=0.05)\n",
    "        enet_reg.fit(X, Y)\n",
    "        coefs.append(enet_reg.coef_)\n",
    "        \n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enet_alphas = np.arange(0.01, 1.0, 0.005)\n",
    "enet_coefs = elasticnet_coefs(X, Y, enet_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enet_plot_runner(alpha=0.01):\n",
    "    coef_plotter(enet_alphas, enet_coefs, X.columns, alpha, regtype='elastic net')\n",
    "\n",
    "interact(enet_plot_runner, alpha=(0.0,1.0,0.005))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-performance'></a>\n",
    "\n",
    "## Model performance of the Ridge, Lasso and Elastic Net on the overfit data\n",
    "\n",
    "---\n",
    "\n",
    "Let's check out how the penalties affect the performance of our models. On the basic wine dataset there won't be much benefit to using regularization. We can switch over to the overfit data instead to see if regularization helps us control overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the KFold crossvalidation function from sklearn. We'll make ten folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# get a generator for the splits\n",
    "kfolds = KFold(n_splits=10, random_state=42).split(X)  \n",
    "\n",
    "# store results as a list of train/test index pairs\n",
    "kfolds = [(train_i, test_i) for train_i, test_i in kfolds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Ridge, Lasso, and Elastic Net, iterate through their alphas and using the cross-validation folds calculate the average $R^2$ at each regularization level:\n",
    "\n",
    "> Note: this may take awhile to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge_lasso_cv_rsq(X, Y, r_alphas, l_alphas, e_alphas, kfolds, verbose=False):\n",
    "    \n",
    "    # lists to track mean R2s per alpha\n",
    "    ridge_rsq = []\n",
    "    lasso_rsq = []\n",
    "    enet_rsq = []\n",
    "\n",
    "    # initialize models\n",
    "    lasso = Lasso()\n",
    "    ridge = Ridge()\n",
    "    enet = ElasticNet()\n",
    "\n",
    "    print('Lasso...')\n",
    "    # iterate through Lasso alphas\n",
    "    for la in l_alphas:\n",
    "        \n",
    "        if verbose: print(la)\n",
    "        \n",
    "        # set the current alpha to the model\n",
    "        lasso.set_params(alpha=la)\n",
    "        \n",
    "        # keep track of fold R2s\n",
    "        rsqs = []\n",
    "        \n",
    "        # iterate through the folds. Each iteration returns the training and\n",
    "        # testing indices\n",
    "        for traini, testi in kfolds:\n",
    "            \n",
    "            # run the current model with the subset training X and Y\n",
    "            lasso.fit(X.iloc[traini, :], Y[traini])\n",
    "            \n",
    "            # append the R2 on the test set to the tracker\n",
    "            rsqs.append(lasso.score(X.iloc[testi, :], Y[testi]))\n",
    "            \n",
    "        # append the mean of the R2s for this alpha to the R2 by alpha list\n",
    "        lasso_rsq.append(np.mean(rsqs))\n",
    "\n",
    "    print('Ridge...')\n",
    "    # Do the same process as above for ridge...\n",
    "    for ra in r_alphas:\n",
    "        \n",
    "        if verbose: print(ra)\n",
    "        \n",
    "        ridge.set_params(alpha=ra)\n",
    "        rsqs = []\n",
    "        for traini, testi in kfolds:\n",
    "            ridge.fit(X.iloc[traini, :], Y[traini])\n",
    "            rsqs.append(ridge.score(X.iloc[testi, :], Y[testi]))\n",
    "        ridge_rsq.append(np.mean(rsqs))\n",
    "        \n",
    "    print('ElasticNet...')\n",
    "    # Do the same process as above for ridge...\n",
    "    for ea in e_alphas:\n",
    "        \n",
    "        if verbose: print(ea)\n",
    "        \n",
    "        enet.set_params(alpha=ea, l1_ratio=0.5)\n",
    "        rsqs = []\n",
    "        for traini, testi in kfolds:\n",
    "            enet.fit(X.iloc[traini, :], Y[traini])\n",
    "            rsqs.append(enet.score(X.iloc[testi, :], Y[testi]))\n",
    "        enet_rsq.append(np.mean(rsqs))\n",
    "        \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    if verbose: print('Fitting')\n",
    "    \n",
    "    linreg_rsq = []\n",
    "    linreg = LinearRegression()\n",
    "    for traini, testi in kfolds:\n",
    "        linreg.fit(X.iloc[traini, :], Y[traini])\n",
    "        linreg_rsq.append(linreg.score(X.iloc[testi, :], Y[[testi]]))\n",
    "        \n",
    "    linreg_rsq = np.mean(linreg_rsq)\n",
    "        \n",
    "    return ridge_rsq, lasso_rsq, enet_rsq, linreg_rsq\n",
    "    \n",
    "# Get the ridge and lasso cross-validated R2s:\n",
    "ridge_rsq, lasso_rsq, enet_rsq, linreg_rsq = ridge_lasso_cv_rsq(Xoverfit, Yoverfit, r_alphas, \n",
    "                                                                l_alphas, enet_alphas, kfolds, verbose=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have a new plotting function that will track the performance of the model as the alphas increase, as measured by the mean $R^2$s across cross-validation folds. Remember that $R^2$ is a measure of how much variance in the target/dependent variable is explained by our predictor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rsq_plotter(ridge_alphas, ridge_to_alpha, ridge_rsq,\n",
    "                lasso_alphas, lasso_to_alpha, lasso_rsq,\n",
    "                enet_alphas, enet_to_alpha, enet_rsq,\n",
    "                linreg_rsq):\n",
    "    \n",
    "    \n",
    "    # Find the overall minimum and maximum alpha values for\n",
    "    # the Ridge and Lasso to fix the plot axes:\n",
    "    ridge_amin = np.min(ridge_alphas)\n",
    "    ridge_amax = np.max(ridge_alphas)\n",
    "    \n",
    "    lasso_amin = np.min(lasso_alphas)\n",
    "    lasso_amax = np.max(lasso_alphas)\n",
    "    \n",
    "    enet_amin = np.min(enet_alphas)\n",
    "    enet_amax = np.max(enet_alphas)\n",
    "    \n",
    "    # Subet the models' alphas and rsqs according to the currently set\n",
    "    # alpha limits for each (passed in from the interactive sliders)\n",
    "    ridge_alphas = [a for a in ridge_alphas if a <= ridge_to_alpha]\n",
    "    ridge_rsq = ridge_rsq[0:len(ridge_alphas)]\n",
    "    \n",
    "    lasso_alphas = [a for a in lasso_alphas if a <= lasso_to_alpha]\n",
    "    lasso_rsq = lasso_rsq[0:len(lasso_alphas)]\n",
    "    \n",
    "    enet_alphas = [a for a in enet_alphas if a <= enet_to_alpha]\n",
    "    enet_rsq = enet_rsq[0:len(enet_alphas)]\n",
    "    \n",
    "    # Get some unique colors out for the Ridge R2 line, Lasso R2 line,\n",
    "    # Enet R2, and the 'max R2 achieved' line.\n",
    "    colors = sns.xkcd_palette(['windows blue', 'amber', 'dark red', 'faded green'])\n",
    "\n",
    "    # We will again be plotting two axes on the same figure:\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(18,7)\n",
    "\n",
    "    # The first subplot axes is for the ridge\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    \n",
    "    # Plot a vertical line indicating the current alpha selected\n",
    "    ax1.axvline(ridge_to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    \n",
    "    # Plot a horizontal line for the vanilla linear regression R^2\n",
    "    ax1.axhline(linreg_rsq, lw=2, c=colors[2], alpha=0.8)\n",
    "    \n",
    "    # Plot the line tracking R2s by alpha values\n",
    "    ax1.plot(ridge_alphas, ridge_rsq, lw=3, c=colors[0])\n",
    "    \n",
    "    # Add the axis labels\n",
    "    ax1.set_xlabel('ridge alpha', fontsize=20)\n",
    "    ax1.set_ylabel('ridge CV R2', fontsize=20)\n",
    "    \n",
    "    # Set x-axis to logarithmic scale\n",
    "    ax1.set_xscale('log')\n",
    "    \n",
    "    # Fix the axes in place\n",
    "    ax1.set_xlim([ridge_amin, ridge_amax])\n",
    "    ax1.set_ylim([-0.05, 1])\n",
    "    \n",
    "    # set the title for the axes;\n",
    "    ax1.set_title('ridge cross-val performance\\n', fontsize=20)\n",
    "    \n",
    "    \n",
    "    # Now do all of this as above for the Lasso!\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.axvline(lasso_to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    ax2.axhline(linreg_rsq, lw=2, c=colors[2], alpha=0.8)\n",
    "    \n",
    "    ax2.plot(lasso_alphas, lasso_rsq, lw=3, c=colors[0])\n",
    "    \n",
    "    ax2.set_xlabel('lasso alpha', fontsize=20)\n",
    "    ax2.set_ylabel('lasso CV R2', fontsize=20)\n",
    "    ax2.set_xlim([lasso_amin, lasso_amax])\n",
    "    ax2.set_ylim([-0.05, 1])\n",
    "        \n",
    "    ax2.set_title('lasso cross-val performance\\n', fontsize=20)\n",
    "    \n",
    "    \n",
    "    # And ElasticNet:\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.axvline(enet_to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
    "    ax3.axhline(linreg_rsq, lw=2, c=colors[3], alpha=0.8)\n",
    "    \n",
    "    ax3.plot(enet_alphas, enet_rsq, lw=3, c=colors[0])\n",
    "    \n",
    "    ax3.set_xlabel('enet alpha', fontsize=20)\n",
    "    ax3.set_ylabel('enet CV R2', fontsize=20)\n",
    "    ax3.set_xlim([enet_amin, enet_amax])\n",
    "    ax3.set_ylim([-0.05, 1])\n",
    "        \n",
    "    ax3.set_title('enet cross-val performance\\n', fontsize=20)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We have our new widget where you can change both the Ridge and Lasso alphas to see how they compare:\n",
    "\n",
    "def rsq_plot_pipe(ra, la, ea):\n",
    "    rsq_plotter(r_alphas, 10**ra, ridge_rsq, l_alphas, la, lasso_rsq, enet_alphas, ea, enet_rsq, linreg_rsq)\n",
    "    \n",
    "w = widgets.interactive(rsq_plot_pipe, \n",
    "                        ra=widgets.FloatSlider(value=0, min=0., max=5., step=0.05, description='Ridge log10(alpha):'),\n",
    "                        la=widgets.FloatSlider(value=0, min=0., max=0.2, step=0.0025, description='Lasso alpha:'),\n",
    "                        ea=widgets.FloatSlider(value=0, min=0., max=1.0, step=0.005, description='Enet alpha:')\n",
    ")\n",
    "\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "widgets": {
   "state": {
    "03860d2d80204ca295d01e93e8e99474": {
     "views": [
      {
       "cell_index": 41
      }
     ]
    },
    "b535fb165fa343b297ba42fb4a55c6fa": {
     "views": [
      {
       "cell_index": 34
      }
     ]
    },
    "f5d5ef714eee4c61b085a3bb6b96cd73": {
     "views": [
      {
       "cell_index": 55
      }
     ]
    },
    "fdc5e91596ea49fa84bf4aed6d37b849": {
     "views": [
      {
       "cell_index": 47
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
